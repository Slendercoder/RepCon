{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/estudiantes/.local/lib/python3.8/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk import Tree\n",
    "from nltk.tree import ParentedTree\n",
    "import es_core_news_sm\n",
    "#import stanza\n",
    "#from spacy_stanza import StanzaLanguage\n",
    "\n",
    "#nlp = es_core_news_sm.load()\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "#snlp = stanza.Pipeline(lang=\"es\")\n",
    "#nlp = StanzaLanguage(snlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_ + '-' + node.dep_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_ + '-' + node.dep_\n",
    "\n",
    "    \n",
    "def to_nltk_tree1(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.tag_, [to_nltk_tree1(child) for child in node.children])\n",
    "    else:\n",
    "        return node.tag_\n",
    "\n",
    "    \n",
    "def crear_arbol(texto, nlp):\n",
    "    document = nlp(texto)\n",
    "    jc = list(document.sents)[0]\n",
    "    arbol = to_nltk_tree(jc.root)\n",
    "    # arbol = ParentedTree.convert(arbol)\n",
    "    arbol.pretty_print()\n",
    "    return arbol\n",
    "\n",
    "\n",
    "def rec1(A):\n",
    "    if type(A) is nltk.Tree:\n",
    "        cadena = A.label() \n",
    "        for i, B in enumerate(A):\n",
    "            cadena += '(' + rec1(B) + ')'\n",
    "        return cadena\n",
    "    else:\n",
    "        return A\n",
    "    \n",
    "    \n",
    "def aplicacion(A, diccionario):\n",
    "    if type(A) is nltk.Tree:\n",
    "        funcion = diccionario[A.label()]\n",
    "        argumentos = [aplicacion(x, diccionario) for x in A]\n",
    "        return funcion(*argumentos)\n",
    "    else:\n",
    "        return diccionario[A]\n",
    "\n",
    "    \n",
    "def limpiar_texto(texto, stopwords):\n",
    "#     token = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "#     tokens = token.tokenize(texto)\n",
    "#     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#    snlp = stanza.Pipeline(lang=\"es\")\n",
    "#    nlp = StanzaLanguage(snlp)\n",
    "#    texto = nlp(texto)\n",
    "    texto = texto.replace('.', '')\n",
    "    lista_texto = texto.split()\n",
    "#    lista = [token.lemma_ for token in texto]\n",
    "    lista = lista_texto\n",
    "    lista = [x for x in lista if x not in stopwords]\n",
    "    return ' '.join(lista)\n",
    "\n",
    "# Función que retorna la forma lógica de un átomo\n",
    "def atomo(texto, nlp, diccionario):\n",
    "    arbol = crear_arbol(texto, nlp)\n",
    "    return aplicacion(arbol, diccionario)\n",
    "\n",
    "# Función para separar por y/o\n",
    "def prop(texto, nlp, diccionario):\n",
    "    documento = nlp(texto)\n",
    "    palabras = [t.orth_ for t in documento]\n",
    "    for i,token in enumerate(documento):\n",
    "#        print(i, token.tag_, token)\n",
    "        if token.tag_ == 'CCONJ':\n",
    "            form1 = atomo(' '.join(palabras[:i]), nlp, diccionario)\n",
    "            form2 = atomo(' '.join(palabras[i+1:]), nlp, diccionario)\n",
    "            if str(token) == 'y':\n",
    "                conectivo = 'Y'\n",
    "            elif str(token) == 'o':\n",
    "                conectivo = 'O'\n",
    "            else:\n",
    "                raise Exception('Conectivo desconocido', token)\n",
    "            return form1 + conectivo + form2\n",
    "    return atomo(texto, nlp, diccionario)\n",
    "\n",
    "# Función que incluye condicionales\n",
    "def traducir(texto, nlp, diccionario):\n",
    "    documento = nlp(texto)\n",
    "    palabras = [t.orth_ for t in documento]\n",
    "    implicacion = False\n",
    "    coma = False\n",
    "    entonces = True\n",
    "    for i,token in enumerate(documento):\n",
    "        if token.tag_ == \"SCONJ\":\n",
    "            ind_si = i\n",
    "            implicacion = True\n",
    "        if implicacion:\n",
    "            if token.tag_ == \"PUNCT\" and token.orth_ == \",\":\n",
    "                ind_coma = i\n",
    "                coma = True\n",
    "            if token.tag_ == \"ADV\" and token.orth_ == \"entonces\":\n",
    "                ind_ent = i\n",
    "                entonces = True\n",
    "    if implicacion and (ind_coma == ind_ent - 1):\n",
    "        Antecedente = prop(' '.join(palabras[ind_si + 1 : ind_coma]), nlp, diccionario)\n",
    "        Consecuente = prop(' '.join(palabras[ind_ent +1:]), nlp, diccionario)\n",
    "        return Antecedente + '>' + Consecuente\n",
    "    return prop(texto, nlp, diccionario)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tipos de Entidades\n",
    "1. Personas\n",
    "2. Objetos\n",
    "3. Lugares\n",
    "4. Tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = {'camina-ROOT': lambda x: 'CAMINAR(' + str(x) + ')',\n",
    "               'está-ROOT': lambda x, y: 'ESTAR(' + str(x) + ',' + str(y) + ')', \n",
    "               'detesta-ROOT': lambda x, y: 'DETESTAR(' + str(x) + ',' + str(y) + ')', \n",
    "               'viaja-ROOT' : lambda x,y,z,w: 'VIAJAR(' + str(x) + ',' + str(y) + ',' + str(z) + ',' + str(w) + ')',\n",
    "               'tiene-ROOT': lambda x, y: 'TENER(' + str(x) + ',' + str(y) + ')',\n",
    "               'tener-ROOT': lambda x, y: 'TENER(' + str(x) + ',' + str(y) + ')',\n",
    "               'comer-ccomp': lambda x: 'COMER(' + str(x) + ')',\n",
    "               'juan-nsubj':'j1',\n",
    "               'laptop-nsubj': 'l2',\n",
    "               'maria-obj':'m1',\n",
    "               'pizza-obj' : 'p2',\n",
    "               'laptop-obj': 'l2',\n",
    "              'Bogotá-nsubj': 'b3',\n",
    "              'Bogotá-obj': 'b3',\n",
    "              'Medellín-obj': 'm3',\n",
    "              'bogota-obj': 'b3',\n",
    "              'medellin-obj': 'm3',\n",
    "              'martes-obj': 'm4',\n",
    "              'miercoles-obj': 'c4',\n",
    "              }\n",
    "\n",
    "stopwords = ['la', 'en', 'a', 'una', 'el', 'de', 'un']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Primero se preprocesa el texto. (?)\n",
    "* Crear el diccionario de forma automática. (?)\n",
    "* Segmentar el texto en frases. (funcion?)\n",
    "* Axiomas del dominio y sentido comúm.\n",
    "* Representación de preguntas.\n",
    "* Crear la base de conocimiento.\n",
    "* Hacer la deducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token attributes: https://spacy.io/api/token\n",
    "\n",
    "*pos_:* Coarse-grained part-of-speech from the Universal POS tag set.\n",
    "\n",
    "*tag_:* Fine-grained part-of-speech.\n",
    "\n",
    "*dep_:* Syntactic dependency relation. \n",
    "\n",
    "*orth_:* ID of the verbatim text content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TENER(j1,l1)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'juan tiene una laptop'\n",
    "texto = limpiar_texto(texto, stopwords)\n",
    "traducir(texto, nlp, diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TENER(j1,l1)YESTAR(j1,b2)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'juan tiene una laptop y juan está en Bogotá'\n",
    "texto = limpiar_texto(texto, stopwords)\n",
    "traducir(texto, nlp, diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercer ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           viaja-ROOT                        \n",
      "     __________|_______________________       \n",
      "juan-nsubj bogota-obj medellin-obj martes-obj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VIAJAR(j1,b2,m2,m3)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'juan viaja bogota medellin martes'\n",
    "texto = limpiar_texto(texto, stopwords)\n",
    "traducir(texto, nlp, diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuarto ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n",
      "             está-ROOT             \n",
      "      ___________|__________        \n",
      "laptop-nsubj           Bogotá-nsubj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ESTAR(j1,b2)>ESTAR(l1,b2)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = 'si juan está en Bogotá, entonces la laptop está en Bogotá'\n",
    "texto = limpiar_texto(texto, stopwords)\n",
    "traducir(texto, nlp, diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quinto ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n",
      "             está-ROOT             \n",
      "      ___________|__________        \n",
      "laptop-nsubj           Bogotá-nsubj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TENER(j1,l1)YESTAR(j1,b2)>ESTAR(l1,b2)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Si juan tiene una laptop y juan está en Bogotá, entonces la laptop está en Bogotá\"\n",
    "txt = limpiar_texto(txt, stopwords)\n",
    "traducir(txt, nlp, diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "           viaja-ROOT                        \n",
      "     __________|_______________________       \n",
      "juan-nsubj bogota-obj medellin-obj martes-obj\n",
      "\n",
      "['TENER(j1,l1)', 'VIAJAR(j1,b2,m2,m3)']\n"
     ]
    }
   ],
   "source": [
    "texto = 'juan tiene una laptop. juan viaja de bogota a medellin el martes'\n",
    "documento = texto.split('. ')\n",
    "formulas_texto = []\n",
    "for s in documento:\n",
    "    txt = limpiar_texto(s, stopwords)\n",
    "    formulas_texto.append(traducir(str(txt), nlp, diccionario))\n",
    "print(formulas_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n",
      "             está-ROOT             \n",
      "      ___________|__________        \n",
      "laptop-nsubj           Bogotá-nsubj\n",
      "\n",
      "['TENER(j1,l1)YESTAR(j1,b2)>ESTAR(l1,b2)']\n"
     ]
    }
   ],
   "source": [
    "sentido_comun = 'Si juan tiene una laptop y juan está en Bogotá, entonces la laptop está en Bogotá'\n",
    "documento = sentido_comun.split('. ')\n",
    "formulas_sentido_comun = []\n",
    "for s in documento:\n",
    "    txt = limpiar_texto(s, stopwords)\n",
    "    formulas_sentido_comun.append(traducir(str(txt), nlp, diccionario))\n",
    "print(formulas_sentido_comun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "Al encontrar preguntas con \"Dónde\" se debe realizar una lista con las posibles respuestas de lugares que hagan verdadera la proposición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "Entidades ['juan-nsubj', 'laptop-nsubj', 'maria-obj', 'pizza-obj', 'laptop-obj', 'Bogotá-nsubj', 'Bogotá-obj', 'Medellín-obj', 'bogota-obj', 'medellin-obj', 'martes-obj', 'miercoles-obj']\n",
      "Lugares ['Bogotá', 'bogota', 'medellin', 'Medellín']\n",
      "        está-ROOT                     \n",
      "    ________|_____________________     \n",
      "¿-punct dónde-obl Bogotá-nsubj ?-punct\n",
      "\n",
      "['punct', 'obl', 'ROOT', 'nsubj', 'punct']\n",
      "['Bogotá está Bogotá', 'Bogotá está bogota', 'Bogotá está medellin', 'Bogotá está Medellín']\n"
     ]
    }
   ],
   "source": [
    "# txt = 'dónde VERBO SUJETO?'\n",
    "txt = '¿dónde está Bogotá?'\n",
    "\n",
    "def pregunta_donde(txt):\n",
    "    tipo_funcion = type(lambda x: x2)\n",
    "    print(tipo_funcion)\n",
    "    Entidades = [k for k in diccionario.keys() if type(diccionario[k]) != tipo_funcion]\n",
    "    print(\"Entidades\", Entidades)\n",
    "    Lugares = [k for k in Entidades if '2' in diccionario[k]]\n",
    "    Lugares = list(set([k.split(\"-\")[0] for k in Lugares]))\n",
    "    print(\"Lugares\", Lugares)\n",
    "    document = nlp(txt)\n",
    "    crear_arbol(txt, nlp)\n",
    "    sujetos = [t.orth_ for t in document if t.tag_ == 'NOUN' or t.tag_ == 'PROPN']\n",
    "    verbs = [t.orth_ for t in document if t.tag_ == 'VERB']\n",
    "    tags = [t.dep_ for t in document]\n",
    "    print(tags)\n",
    "    props = []\n",
    "    for x2 in Lugares:\n",
    "        props.append(f'{sujetos[0]} {verbs[0]} {x2}')\n",
    "    return props\n",
    "\n",
    "print(pregunta_donde(txt))\n",
    "# 'dónde está el laptop?' => 'el laptop está x2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('donde', 'PRON', 'obl'), ('está', 'VERB', 'ROOT'), ('libro', 'NOUN', 'nsubj')]\n",
      "está\n",
      "          está-ROOT            \n",
      "     _________|__________       \n",
      "donde-obl           libro-nsubj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texto = 'donde está el libro'\n",
    "texto = limpiar_texto(texto, stopwords)\n",
    "document = nlp(texto)\n",
    "palabras = [(t.orth_, t.tag_, t.dep_) for t in document]\n",
    "print(palabras)\n",
    "jc = list(document.sents)[0]\n",
    "print(jc.root)\n",
    "arbol = to_nltk_tree(jc.root)\n",
    "#arbol = ParentedTree.convert(arbol)\n",
    "arbol.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de conocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan tiene laptop\n",
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "juan está Bogotá\n",
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n",
      "Si juan está Bogotá y juan tiene laptop, entonces laptop está Bogotá\n",
      "           está-ROOT             \n",
      "     __________|__________        \n",
      "juan-nsubj           Bogotá-nsubj\n",
      "\n",
      "           tiene-ROOT           \n",
      "     __________|__________       \n",
      "juan-nsubj            laptop-obj\n",
      "\n",
      "             está-ROOT             \n",
      "      ___________|__________        \n",
      "laptop-nsubj           Bogotá-nsubj\n",
      "\n",
      "['TENER(j1,l1)', 'ESTAR(j1,b2)', 'ESTAR(j1,b2)YTENER(j1,l1)>ESTAR(l1,b2)']\n"
     ]
    }
   ],
   "source": [
    "texto = 'juan tiene un laptop. juan está en Bogotá.'\n",
    "axiomas = 'Si juan está en Bogotá y juan tiene un laptop, entonces la laptop está en Bogotá.'\n",
    "formulas = []\n",
    "doc = nlp(texto + axiomas)\n",
    "\n",
    "for s in doc.sents:\n",
    "    sentence = limpiar_texto(s.text, stopwords)\n",
    "    print(sentence)\n",
    "    formulas.append(traducir(sentence, nlp, diccionario))\n",
    "\n",
    "print(formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos:\n",
      "TENER(j1,l1)\n",
      "ESTAR(j1,b2)\n",
      "\n",
      "Reglas:\n",
      "ESTAR(j1,b2)YTENER(j1,l1)>ESTAR(l1,b2)\n",
      "\n",
      "<class 'function'>\n",
      "Entidades ['juan-nsubj', 'laptop-nsubj', 'maria-obj', 'pizza-obj', 'laptop-obj', 'Bogotá-nsubj', 'Bogotá-obj', 'Medellín-obj', 'bogota-obj', 'medellin-obj', 'martes-obj', 'miercoles-obj']\n",
      "Lugares ['Bogotá', 'bogota', 'medellin', 'Medellín']\n",
      "          está-ROOT             \n",
      "     _________|__________        \n",
      "    |         |     laptop-nsubj\n",
      "    |         |          |       \n",
      "dónde-obl  ?-punct     el-det   \n",
      "\n",
      "['obl', 'ROOT', 'det', 'nsubj', 'punct']\n",
      "             está-ROOT             \n",
      "      ___________|__________        \n",
      "laptop-nsubj           Bogotá-nsubj\n",
      "\n",
      "ESTAR(l1,b2)\n",
      "Respuesta: laptop está Bogotá\n"
     ]
    }
   ],
   "source": [
    "from logica import *\n",
    "\n",
    "# texto + axiomas => formulas de la base\n",
    "\n",
    "b = LPQuery(formulas)\n",
    "print(b)\n",
    "pregunta = 'dónde está el laptop?'\n",
    "\n",
    "rep_pregunta = pregunta_donde(pregunta)\n",
    "for o in rep_pregunta:\n",
    "    opcion = traducir(o, nlp, diccionario)\n",
    "    print(opcion)\n",
    "    if ASK(opcion,'success',b):\n",
    "        print('Respuesta:', o)\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototipos = {\n",
    "   'X1': 'juan',\n",
    "    'X2' : 'laptop',\n",
    "    'X3' : 'Bogotá',\n",
    "    'X4' : 'martes'\n",
    "}\n",
    "\n",
    "prots = lambda x : prototipos[x] if x in prototipos else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['si',\n",
       " 'juan',\n",
       " 'tiene',\n",
       " 'laptop',\n",
       " 'y',\n",
       " 'juan',\n",
       " 'está',\n",
       " 'en',\n",
       " 'X3,',\n",
       " 'entonces',\n",
       " 'laptop',\n",
       " 'está',\n",
       " 'en',\n",
       " 'Bogotá']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'si X1 tiene X2 y X1 está en X3, entonces X2 está en X3'\n",
    "list(map(prots, txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tiene-ROOT      \n",
      "    ________|________    \n",
      "1-nsubj            2-obj\n",
      "\n",
      "está-ROOT\n",
      "    |     \n",
      " 1-nsubj \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ee5bb6edb20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'si 1 tiene 2 y 1 está en 3, entonces 2 está en 3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraducir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-7ca8075d6c10>\u001b[0m in \u001b[0;36mtraducir\u001b[0;34m(texto, nlp, diccionario)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mentonces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimplicacion\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind_coma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mind_ent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mAntecedente\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_si\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mind_coma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mConsecuente\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_ent\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAntecedente\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'>'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mConsecuente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7ca8075d6c10>\u001b[0m in \u001b[0;36mprop\u001b[0;34m(texto, nlp, diccionario)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CCONJ'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mform1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matomo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mform2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matomo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mconectivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7ca8075d6c10>\u001b[0m in \u001b[0;36matomo\u001b[0;34m(texto, nlp, diccionario)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0matomo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0marbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrear_arbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maplicacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Función para separar por y/o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7ca8075d6c10>\u001b[0m in \u001b[0;36maplicacion\u001b[0;34m(A, diccionario)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfuncion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0margumentos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maplicacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuncion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margumentos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# F\n",
    "def desgeneralizar(txt):\n",
    "    pass\n",
    "\n",
    "\n",
    "traducir(txt, nlp, diccionario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
